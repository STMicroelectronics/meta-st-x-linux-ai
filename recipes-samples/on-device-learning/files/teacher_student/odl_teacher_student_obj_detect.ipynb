{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-device Learning: Teacher-Student use case using Transfer Learning for object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Introduction\n",
    "### 1. Teacher-Student Machine Learning:\n",
    "Teacher-student machine learning with automatic labeling is an advanced technique where a large, well-trained model (the teacher) is used to generate labels for a dataset that lacks annotations. The teacher model predicts labels for the unlabeled data, which are then used to train a smaller, simpler model (the student). This process allows the student model to learn from the teacher's knowledge, effectively transferring the teacher's expertise to the student. This method is particularly useful for creating efficient models for deployment on resource-constrained devices, while also reducing the need for manual data labeling.\n",
    "\n",
    "### 2. Transfer Learning:\n",
    "Transfer learning is a machine learning technique where a pre-trained model, developed for one task, is adapted to perform a different but related task. Instead of training a model from scratch, transfer learning leverages the knowledge gained from the initial task to improve the performance and efficiency of the new task. This approach is particularly useful when there is limited data available for the new task, as it allows the model to benefit from the extensive training of the pre-trained model. This tutorial showcases the transfer learning technique where knowledge gained from training a model on one task is leveraged to improve the performance of a model on a different but related task. Instead of starting the learning process from scratch, transfer learning allows us to transfer the knowledge or features learned by a pre-trained model to a new task.\n",
    "\n",
    "### 3. Introduction of the use case:\n",
    "In this notebook, we will demonstrate the concept of transfer learning using an example dataset and evaluate the ORT training API on an **STM32MP257** device. We will employ a custom dataset where data is directly **retrieved**, **processed**, and **labeled** for training directly on the device. For this tutorial, we will leverage the SSD MobileNetV2 model which has been trained on large-scale image datasets such as PASCAL VOC for object detection (which has 20 classes). We will use this model for detecting custom data into one class. The class for this use case will be person but the user is free to adapt the use case into his needs. The initial layers of SSD MobileNetV2 serve as a feature extractor, capturing generic visual features applicable to various tasks, and only the final layer will be trained for the task at hand. The figure below summarizes the whole workflow followed to realize this Teacher Student Machine Learning workflow:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"/usr/local/x-linux-ai/resources/ODL_teacher_student_workflow.png\">\n",
    "</div>\n",
    "\n",
    "### 4. Prerequisites:\n",
    "In order to be able to retrain the model using Onnxruntime training API, it is mandatory to generate the training artifacts:\n",
    "*  `Training model (onnx.ModelProto)`: Contains the base model graph, loss subgraph and the gradient graph.\n",
    "*  `Eval Model (onnx.ModelProto)`: Contains the base model graph and the loss subgraph.\n",
    "*  `Optimizer Model (onnx.ModelProto)`: Contains the optimizer graph.\n",
    "*  `Checkpoint (Directory)`: Contains the model parameters split into 2 .pbsec files, one for frozen parameters and one for trainable parameters.\n",
    "These artifacts can be generated by running the following [the dedicated wiki article](https://wiki.st.com/stm32mpu/wiki/How_to_generate_training_artifacts_for_on-device_learning_feature#) on your host computer. These artifacts should be then deployed on the **STM32MP257** board using either the `scp` tool or the drag and drop functionality provided by this Jupyter-lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________\n",
    "## II. Data collection using on-device camera IMX335 sensor\n",
    "The primary advantage of on-device learning is that the data remains on the device, ensuring enhanced privacy and security. This approach eliminates the need to transfer sensitive data to external servers for processing, thereby reducing the risk of data breaches and unauthorized access. It allows for personalized models that can adapt to individual user behavior and preferences without compromising data security. Therefore the need to use on-device camera sensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "import supervision as sv\n",
    "\n",
    "# Widget libraries for interaction with user\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import threading\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUPYTER_ROOT_PATH = \"/usr/local/x-linux-ai/on-device-learning/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Raw data collection\n",
    "In th process of data collection, it is crucial to ensure that the data is generic and uniform across all classes. This means that the dataset should be balanced, with an equal or proportionate number of samples for each class to prevent bias towar,\n",
    "In the next cells start by creating the directory where all the raw data will be stored. Please make sure that your disk partition has enough space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_images = 50                  # Number of images to be retrieved\n",
    "retrieval_frequency = 5                     # Number of frames between two retrieved frames\n",
    "input_height = 240                          # Height of frames to be displayed\n",
    "input_width  = 320                          # Width of frames to be displayed\n",
    "input_nn_width = 256                        # NN input width\n",
    "input_nn_height = 256                       # NN input height\n",
    "dataset_dir = JUPYTER_ROOT_PATH + \"data\"\n",
    "if (os.path.exists(dataset_dir) == False):\n",
    "    os.mkdir(dataset_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widgets for interaction with the user\n",
    "# ================\n",
    "stopButton = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Stop',\n",
    "    disabled=False,\n",
    "    button_style='danger', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Description',\n",
    "    icon='square' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "startRetrieval = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Start data retrieval',\n",
    "    disabled=False,\n",
    "    button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Description',\n",
    "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "progressBar = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=1,\n",
    "    max=num_samples_images,\n",
    "    description='Data collection:',\n",
    "    bar_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    orientation='horizontal'\n",
    ")\n",
    "\n",
    "class_box = widgets.HBox([startRetrieval, progressBar, stopButton])\n",
    "\n",
    "# Display function\n",
    "# ================\n",
    "def preview(button):\n",
    "    from PIL import Image\n",
    "    # Define the GStreamer pipeline\n",
    "    gst_pipeline = (\n",
    "        \"libcamerasrc ! \"\n",
    "        \"video/x-raw,width={frame_width},height={frame_height},format=RGB16 !\"\n",
    "        \"queue leaky=2 max-size-buffers=1 !\"\n",
    "        \"videoconvert !\"\n",
    "        \"appsink\"\n",
    "    ).format(frame_width=640, frame_height=480)\n",
    "\n",
    "    # Use the pipeline with cv2.VideoCapture\n",
    "    cap = cv2.VideoCapture(gst_pipeline, cv2.CAP_GSTREAMER)\n",
    "    display_handle=display(None, display_id=True)\n",
    "    image_idx = 0\n",
    "    class_idx = 0\n",
    "    while True:\n",
    "        _, frame = cap.read()\n",
    "        frame_cvt = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rsz = cv2.resize(np.array(frame_cvt), (input_width, input_height))\n",
    "        display_handle.update(Image.fromarray(frame_rsz))\n",
    "        if startRetrieval.value == True:\n",
    "            if image_idx < num_samples_images:\n",
    "                image_path = dataset_dir + '/' + 'image_' +str(image_idx) + '.jpg'\n",
    "                frame_saved = cv2.resize(np.array(frame), (input_nn_width, input_nn_height))\n",
    "                cv2.imwrite(image_path, frame_saved)\n",
    "                image_idx = image_idx + 1\n",
    "                progressBar.value = image_idx\n",
    "            if image_idx == num_samples_images:\n",
    "                image_idx = 0\n",
    "                startRetrieval.value = False\n",
    "                class_idx = class_idx + 1\n",
    "                print(\"Data collected successfully\")\n",
    "        if stopButton.value==True:\n",
    "            cap.release()\n",
    "            display_handle.update(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(class_box)\n",
    "thread = threading.Thread(target=preview, args=(stopButton,))\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________\n",
    "## III. Preparing dataset in PASCAL VOC format with automatic labeling using Teacher Model\n",
    "Real-Time Detection Transformer **(RT-DETR)**, developed by Baidu, is a cutting-edge end-to-end object detector that provides real-time performance while maintaining high accuracy. It is based on the idea of DETR (the NMS-free framework), meanwhile introducing conv-based backbone and an efficient hybrid encoder to gain real-time speed. For this use case, **RT-DETR**  is used as a teacher model because of its high accuracy in automatic labeling by leveraging its robust detection capabilities to generate precise annotations for unlabeled datasets. This model can identify and classify objects in images and videos with remarkable speed and accuracy, making it an ideal choice for creating labeled datasets without extensive manual effort. By using **RT-DETR** as a teacher model, the generated labels can then be used to train smaller, student models, facilitating the development of efficient and high-performing machine learning systems. </br>\n",
    "\n",
    "In order to annotate the collected data, we will need to run inferences using the **STAI_MPU API** and the **RT-DETR** model exported previously to ONNX format. Some of the model's operator are not supported by the NPU compiler, hence the unique execution provider for these inference session should be the CPU. For these reasons and due to the teacher model size, each of the inferences should take few seconds to reach high accuracy predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stai_mpu import stai_mpu_network\n",
    "import numpy as np\n",
    "\n",
    "confidence_threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Input pre-processing:\n",
    "Below we define the util function `preprocess_input` required to preprocess the input image to fit in the RT-DETRv2 input shape and data type. We apply also some normalization to scale input pixel values between 0 and 1 since the model expects float32 tensor as an input. It is important to mention that this function is used to pre-process the inputs for the student model also but with a different resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(image, input_width, input_height):\n",
    "    img_height, img_width, num_channel = image.shape\n",
    "    input_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    input_img = cv2.resize(input_img, (input_width, input_height))\n",
    "    input_img = input_img.transpose(2, 0, 1)\n",
    "    input_img = (np.float32(input_img) - 127.5) / 127.5\n",
    "    expand_tensor = np.expand_dims(input_img, axis=0)\n",
    "    input_tensor = expand_tensor.copy()\n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Output post-processing:\n",
    "Below we define the util function required to post-process the output tensor, which means applying a serie of transformations to the raw outputs of an object detection model to convert them into meaningful and usable results. These steps typically include filtering, refining, and formatting the model's predictions to produce the final set of detected objects with their associated bounding boxes, class labels, and confidence scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **bbox_cxcywh_to_xyxy** converts bounding boxes from the format *(center_x, center_y, width, height)* to the format *(x_min, y_min, x_max, y_max)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_cxcywh_to_xyxy(boxes):\n",
    "    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
    "    x1 = cx - 0.5 * w\n",
    "    y1 = cy - 0.5 * h\n",
    "    x2 = cx + 0.5 * w\n",
    "    y2 = cy + 0.5 * h\n",
    "    return np.stack([x1, y1, x2, y2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the postprocess_rtdetr function that processes the raw output of an object detection model to produce filtered and formatted bounding boxes, confidence scores, and class labels. This function performs several key steps to ensure the output is usable such as score normalization, confidence thresholding, target label filtering to make sure the model detects only the required classes and finally bounding box clipping. A quick reminder that the **RT-DETR** model is NMS-free. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_rtdetr(output, target_labels=None):\n",
    "        output = np.array(output[0])\n",
    "        boxes, scores = output[:, :4], output[:, 4:]\n",
    "        if not (np.all((scores > 0) & (scores < 1))):\n",
    "            scores = 1 / (1 + np.exp(-scores))\n",
    "        boxes = bbox_cxcywh_to_xyxy(boxes)\n",
    "        _max = scores.max(-1)\n",
    "        _mask = _max > confidence_threshold\n",
    "        boxes, scores = boxes[_mask], scores[_mask]\n",
    "        labels, scores = scores.argmax(-1), scores.max(-1)\n",
    "\n",
    "        # As the model is pretrained on several classes, it may detect unwanted classes\n",
    "        # target_labels allows to select\n",
    "        if target_labels is not None:\n",
    "            # Filter by target label\n",
    "            target_label_indices = [i for i, label in enumerate(\n",
    "                labels) if label in target_labels]\n",
    "\n",
    "            labels = np.array([labels[i] for i in target_label_indices])\n",
    "            boxes = np.array([boxes[i] for i in target_label_indices])\n",
    "            scores = np.array([scores[i] for i in target_label_indices])\n",
    "\n",
    "        if scores.shape[0] == 0:\n",
    "            return [], [], []\n",
    "\n",
    "        x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
    "        x1 = np.floor(np.minimum(np.maximum(1, x1 * input_nn_width), input_nn_width - 1)).astype(int)\n",
    "        y1 = np.floor(np.minimum(np.maximum(1, y1 * input_nn_height), input_nn_height - 1)).astype(int)\n",
    "        x2 = np.ceil(np.minimum(np.maximum(1, x2 * input_nn_width), input_nn_width - 1)).astype(int)\n",
    "        y2 = np.ceil(np.minimum(np.maximum(1, y2 * input_nn_height), input_nn_height - 1)).astype(int)\n",
    "        boxes = np.stack([x1, y1, x2, y2], axis=1)\n",
    "\n",
    "        return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preparing the teacher model instance for inference:\n",
    "To run the inference on the retrieved image, we will be using the STAI_MPU API on the RT-DETR model in ONNX format and the CPU execution engine only since some of the operators are not supported by the NPU Hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_file_path = JUPYTER_ROOT_PATH + \"student_model/ssd_mobilenet_v2/labels.txt\"\n",
    "teacher_model_path = JUPYTER_ROOT_PATH +  \"teacher_model/rt-detr/rtdetr-l.onnx\"\n",
    "stai_teacher_model = stai_mpu_network(model_path=teacher_model_path, use_hw_acceleration=False)\n",
    "\n",
    "# Read input tensor information\n",
    "num_inputs = stai_teacher_model.get_num_inputs()\n",
    "input_tensor_infos = stai_teacher_model.get_input_infos()\n",
    "\n",
    "# Read output tensor information\n",
    "num_outputs = stai_teacher_model.get_num_outputs()\n",
    "output_tensor_infos = stai_teacher_model.get_output_infos()\n",
    "output_tensor_shape = output_tensor_infos[0].get_shape()\n",
    "\n",
    "input_tensor_shape = input_tensor_infos[0].get_shape()\n",
    "input_width =  input_tensor_shape[2]\n",
    "input_height =  input_tensor_shape[3]\n",
    "input_channel =  input_tensor_shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 4. Converting the dataset to PASCAL VOC format:\n",
    "\n",
    "The **format_predictions_for_xml** function is designed to format object detection predictions into a structured list of dictionaries. It takes four parameters: nb_detections (the number of detected objects), class_names (a list of class names), class_ids (a list of class IDs corresponding to the detected objects), and boxes (a list of bounding box coordinates for each detected object). For each detection, the function retrieves the class name using the class ID, extracts the bounding box coordinates, and creates a dictionary with the class name and bounding box coordinates (xmin, xmax, ymin, ymax). These dictionaries are then appended to a list called predictions, which is returned at the end of the function. This list will be used later to generate XML representations of the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_predictions_for_xml(nb_detections, class_names, class_ids, boxes):\n",
    "        predictions = []\n",
    "        for i in range(nb_detections):\n",
    "            name = class_names[class_ids[i]]\n",
    "            bbox = boxes[i]\n",
    "            predictions.append({\n",
    "                'name': name,\n",
    "                'xmin': bbox[0],\n",
    "                'xmax': bbox[2],\n",
    "                'ymin': bbox[1],\n",
    "                'ymax': bbox[3]\n",
    "            })\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **generate_voc_xml_annotation** function creates an XML file in the PASCAL VOC format, which is commonly used for object detection datasets. This function takes various parameters including the image dimensions (width, height, depth), a list of objects with their bounding box coordinates, and the output file path. It generates an XML structure that includes metadata about the image and detailed annotations for each object, such as the object name and bounding box coordinates. The resulting XML file is then written to the specified output path, formatted in a human-readable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_voc_xml_annotation(width, height, depth, objects, output_file, filename=\"NA\", path=\"NA\"):\n",
    "        \"\"\"\n",
    "        Create an XML file in VOC format.\n",
    "\n",
    "        :param filename: Name of the image file\n",
    "        :param path: Path to the image file\n",
    "        :param width: Width of the image\n",
    "        :param height: Height of the image\n",
    "        :param depth: Depth of the image (number of channels)\n",
    "        :param objects: List of objects, where each object is a dictionary with keys:\n",
    "                        'name', 'xmin', 'xmax', 'ymin', 'ymax'\n",
    "        :param output_file: Path to the output XML file\n",
    "        \"\"\"\n",
    "        annotation = ET.Element(\"annotation\")\n",
    "\n",
    "        folder = ET.SubElement(annotation, \"folder\")\n",
    "        folder.text = \"\"\n",
    "\n",
    "        filename_elem = ET.SubElement(annotation, \"filename\")\n",
    "        filename_elem.text = filename\n",
    "        path_elem = ET.SubElement(annotation, \"path\")\n",
    "        path_elem.text = path\n",
    "        source = ET.SubElement(annotation, \"source\")\n",
    "        database = ET.SubElement(source, \"database\")\n",
    "        database.text = \"ST\"\n",
    "        size = ET.SubElement(annotation, \"size\")\n",
    "        width_elem = ET.SubElement(size, \"width\")\n",
    "        width_elem.text = str(width)\n",
    "        height_elem = ET.SubElement(size, \"height\")\n",
    "        height_elem.text = str(height)\n",
    "        depth_elem = ET.SubElement(size, \"depth\")\n",
    "        depth_elem.text = str(depth)\n",
    "        segmented = ET.SubElement(annotation, \"segmented\")\n",
    "        segmented.text = \"0\"\n",
    "        for obj in objects:\n",
    "            object_elem = ET.SubElement(annotation, \"object\")\n",
    "            name = ET.SubElement(object_elem, \"name\")\n",
    "            name.text = obj['name']\n",
    "            pose = ET.SubElement(object_elem, \"pose\")\n",
    "            pose.text = \"Unspecified\"\n",
    "            truncated = ET.SubElement(object_elem, \"truncated\")\n",
    "            truncated.text = \"0\"\n",
    "            difficult = ET.SubElement(object_elem, \"difficult\")\n",
    "            difficult.text = \"0\"\n",
    "            occluded = ET.SubElement(object_elem, \"occluded\")\n",
    "            occluded.text = \"0\"\n",
    "            bndbox = ET.SubElement(object_elem, \"bndbox\")\n",
    "            xmin = ET.SubElement(bndbox, \"xmin\")\n",
    "            xmin.text = str(obj['xmin'])\n",
    "            xmax = ET.SubElement(bndbox, \"xmax\")\n",
    "            xmax.text = str(obj['xmax'])\n",
    "            ymin = ET.SubElement(bndbox, \"ymin\")\n",
    "            ymin.text = str(obj['ymin'])\n",
    "            ymax = ET.SubElement(bndbox, \"ymax\")\n",
    "            ymax.text = str(obj['ymax'])\n",
    "        # Create a new XML file with the results\n",
    "        tree = ET.ElementTree(annotation)\n",
    "        tree.write(output_file)\n",
    "        # Pretty print the XML\n",
    "        xml_str = minidom.parseString(ET.tostring(\n",
    "            annotation)).toprettyxml(indent=\"    \")\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(xml_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Splitting the dataset into train, test and eval sets:\n",
    "Next, we define the `split_dataset` function that organizes a raw dataset of images into three subsets: training, testing, and evaluation. It first creates the necessary directories (train, test, eval) within a specified dataset path. It then lists and filters image files from the raw dataset directory, shuffles them to ensure randomness, and splits them into three groups: 70% for training, 10% for testing, and the remaining 20% for evaluation. Finally, it moves the images to their respective subdirectories and prints a message indicating the completion of the dataset splitting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset_raw_path, dataset_split_path, train_percent=0.7, test_percent=0.1):\n",
    "    # Create dataset directory and subdirectories if they don't exist\n",
    "    train_dir = os.path.join(dataset_split_path, \"train\")\n",
    "    test_dir = os.path.join(dataset_split_path, \"test\")\n",
    "    eval_dir = os.path.join(dataset_split_path, \"eval\")\n",
    "\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "    # Split new_images into train, test, and eval\n",
    "    new_images_dir = dataset_raw_path\n",
    "    image_files = [\n",
    "        os.path.join(new_images_dir, f)\n",
    "        for f in os.listdir(new_images_dir)\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))\n",
    "    ]\n",
    "\n",
    "    random.shuffle(image_files)\n",
    "    total_images = len(image_files)\n",
    "    train_split = int(train_percent * total_images)\n",
    "    test_split = int(test_percent * total_images)\n",
    "    eval_split = total_images - train_split - test_split\n",
    "\n",
    "    train_images = image_files[:train_split]\n",
    "    test_images = image_files[train_split:train_split + test_split]\n",
    "    eval_images = image_files[train_split + test_split:]\n",
    "\n",
    "    # Move images to respective subfolders\n",
    "    for image_path in train_images:\n",
    "        shutil.move(image_path, os.path.join(train_dir, os.path.basename(image_path)))\n",
    "    for image_path in test_images:\n",
    "        shutil.move(image_path, os.path.join(test_dir, os.path.basename(image_path)))\n",
    "    for image_path in eval_images:\n",
    "        shutil.move(image_path, os.path.join(eval_dir, os.path.basename(image_path)))\n",
    "\n",
    "    print(\"Dataset split complete.\")\n",
    "\n",
    "\n",
    "train_percent = 0.7\n",
    "test_percent = 0.1\n",
    "split_dataset(dataset_dir, dataset_dir, train_percent, test_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Generating labels in Pascal VOC format and visualizing annotations:\n",
    "Now that all the util functions required have been defined, we are ready to generate the labels using the teacher model. These labels are intended to be used for the student model training. The following cell consist of running the complete loop by applying the pre-processing on the images, running the inference using the RT-DETR model, applying the post-processing on the output tensor and reformatting it to Pascal VOC XML format to prepare it for training the student model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_idx = 0\n",
    "annotation_progress = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=1,\n",
    "    max=num_samples_images,\n",
    "    description='Data annotation:',\n",
    "    bar_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    orientation='horizontal'\n",
    ")\n",
    "display(annotation_progress)\n",
    "\n",
    "annotations_dir = \"generated_annotations\"\n",
    "class_names = [name.strip() for name in open(labels_file_path).readlines()]\n",
    "\n",
    "if not os.path.exists(annotations_dir):\n",
    "    os.mkdir(annotations_dir)\n",
    "    print(f\"Directory '{annotations_dir}' created.\\n\")\n",
    "else:\n",
    "    print(f\"Directory '{annotations_dir}' already exists.\\n\")\n",
    "\n",
    "for subdir in ['train', 'test', 'eval']:\n",
    "    subdir_path = os.path.join(dataset_dir, subdir)\n",
    "    for img_filename in os.listdir(subdir_path):\n",
    "        if img_filename.lower().endswith((\"jpg\", \"png\", \"jpeg\")):\n",
    "            img = cv2.imread(f\"{subdir_path}/{img_filename}\")\n",
    "            preprocessed_img = preprocess_input(img, input_width, input_height)\n",
    "            stai_teacher_model.set_input(0, preprocessed_img)\n",
    "            stai_teacher_model.run()\n",
    "            output_tensors = stai_teacher_model.get_output(index=0)\n",
    "            #output_tensors = session.run(output_names, {input_names[0]: preprocessed_img})\n",
    "            boxes, scores, class_ids = postprocess_rtdetr(output=output_tensors, target_labels=[0])\n",
    "            if type(class_ids) == list:\n",
    "                print(f\"No object found in {img_filename}\")\n",
    "                nb_detections = 0\n",
    "            else:\n",
    "                nb_detections = class_ids.shape[0]\n",
    "            objects = format_predictions_for_xml(nb_detections, class_names, class_ids, boxes)\n",
    "            img_name_without_extension = os.path.splitext(img_filename)[0]\n",
    "            output_file = f\"{subdir_path}/{img_name_without_extension}.xml\"\n",
    "            generate_voc_xml_annotation(\n",
    "                input_width, input_height, input_channel, objects, output_file, filename=img_filename, path=f\"{subdir_path}/{img_filename}\")\n",
    "            label_idx = label_idx + 1\n",
    "            annotation_progress.value = label_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will visualize the generated labels to ensure their quality and accuracy before proceeding with the student model training. This step involves displaying a subset of the annotated images with their corresponding **bounding boxes**, as predicted by the teacher model. By doing so, we can manually inspect the annotations to verify that the objects are correctly identified and localized. This visual inspection is crucial for identifying any potential errors or inconsistencies in the labeling process, allowing us to make necessary adjustments and ensure that the student model is trained on high-quality data. The following cell will display a grid of annotated images, providing a clear and intuitive overview of the labeling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_voc_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    boxes = []\n",
    "    scores = []\n",
    "    class_ids = []\n",
    "    class_name_to_id = {'person': 1}  # Example mapping, extend as needed\n",
    "\n",
    "    for obj in root.findall('object'):\n",
    "        bbox = obj.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text)\n",
    "        ymin = int(bbox.find('ymin').text)\n",
    "        xmax = int(bbox.find('xmax').text)\n",
    "        ymax = int(bbox.find('ymax').text)\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        scores.append(1.0)  # Assuming score is 1.0 for all detections\n",
    "        class_name = obj.find('name').text\n",
    "        class_ids.append(class_name_to_id.get(class_name, 0))  # Default to 0 if class not found\n",
    "\n",
    "    if not boxes:\n",
    "        boxes = np.empty((0, 4))\n",
    "    else:\n",
    "        boxes = np.array(boxes)\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    class_ids = np.array(class_ids)\n",
    "\n",
    "    return sv.Detections(xyxy=boxes, confidence=scores, class_id=class_ids)\n",
    "\n",
    "def display_annotated_images(image_paths, detections_list):\n",
    "    images = [cv2.imread(path) for path in image_paths]\n",
    "\n",
    "    box_annotator = sv.BoxAnnotator()\n",
    "    annotated_images = []\n",
    "    for image, detections in zip(images, detections_list):\n",
    "        annotated_image = box_annotator.annotate(\n",
    "            scene=image.copy(),\n",
    "            detections=detections\n",
    "        )\n",
    "        annotated_images.append(annotated_image)\n",
    "\n",
    "    # Convert BGR images to RGB for matplotlib\n",
    "    annotated_images_rgb = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in annotated_images]\n",
    "\n",
    "    # Create a figure with 1 row and len(image_paths) columns\n",
    "    fig, axes = plt.subplots(1, len(image_paths), figsize=(20, 5))\n",
    "\n",
    "    # Display each image in the grid\n",
    "    for ax, img in zip(axes, annotated_images_rgb):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')  # Hide axes\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "image_paths = glob.glob(f\"{dataset_dir}/train/*.jpg\")\n",
    "xml_paths = glob.glob(f\"{dataset_dir}/train/*.xml\")\n",
    "\n",
    "detections_list = [parse_voc_xml(xml) for xml in xml_paths]\n",
    "display_annotated_images(image_paths[:5], detections_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Data loader instance for the SSD MobileNetv2:\n",
    "To facilitate the training process of the student model, we need to define a data loader that efficiently handles the loading and batching of our dataset. The data loader will be responsible for reading the images and their corresponding Pascal VOC XML annotations, applying any necessary transformations and augmentations, and organizing the data into batches suitable for training. This ensures that the model receives data in a consistent and optimized manner, improving the training efficiency and performance. The data loader will also handle shuffling the dataset to ensure that the model generalizes well and does not overfit to specific data patterns.  </br> We start by defining a class that applies some transformations on the dataset before feeding it into the dataloader: converting image data to float32, normalizing bounding box coordinates to a percentage of the image dimensions, resizing the image to a specified size, subtracting a mean value from the image for normalization, dividing by a standard deviation, and transposing the image dimensions to have the channel first. This ensures the images are in the correct format and scale for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformData:\n",
    "    def __init__(self, size, mean=0, std=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size: the size of the final image.\n",
    "            mean: mean pixel value per channel.\n",
    "            std: standard deviation for normalization.\n",
    "        \"\"\"\n",
    "        self.mean = np.array(mean, dtype=np.float32)\n",
    "        self.size = size\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, img, boxes, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img: the output of cv.imread in RGB layout.\n",
    "            boxes: bounding boxes in the form of (x1, y1, x2, y2).\n",
    "            labels: labels of boxes.\n",
    "        \"\"\"\n",
    "        img = img.astype(np.float32)\n",
    "        height, width, _ = img.shape\n",
    "        if boxes.size != 0:\n",
    "            boxes[:, 0] /= width\n",
    "            boxes[:, 2] /= width\n",
    "            boxes[:, 1] /= height\n",
    "            boxes[:, 3] /= height\n",
    "        img = cv2.resize(img, (self.size, self.size))\n",
    "        img -= self.mean\n",
    "        img /= self.std\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "\n",
    "        return img, boxes, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will define the data loader imported from torch library, for that purpose we will be needing to define a Custom dataset that follows PASCAL VOC formatting by detailing the steps for reading the data, applying transformations, and batching the images and labels for the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "class VOCDataset:\n",
    "    def __init__(self, root, dataset_type='train', transform=None, target_transform=None, label_file=None):\n",
    "        \"\"\"Dataset for VOC data.\n",
    "        Args:\n",
    "            root: the root of the dataset, the directory contains the following sub-directories:\n",
    "                Annotations, test, eval, and train.\n",
    "            dataset_type: specify the dataset type ('train', 'test', or 'eval').\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.dataset_type = dataset_type\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        # Determine the image directory based on the dataset type\n",
    "        image_dir = self.dataset_type\n",
    "        files = glob.glob(os.path.join(self.root, image_dir, \"*.[jp][pn]g\"))\n",
    "        self.ids = [Path(file).stem for file in files]\n",
    "\n",
    "        # Read class names from the labels file if it exists\n",
    "        if os.path.isfile(labels_file_path):\n",
    "            with open(labels_file_path, 'r') as infile:\n",
    "                classes = [line.strip() for line in infile]\n",
    "            classes.insert(0, 'BACKGROUND')\n",
    "            self.class_names = tuple(classes)\n",
    "            logging.info(\"VOC Labels read from file: \" + str(self.class_names))\n",
    "        else:\n",
    "            logging.info(\"No labels file, using default VOC classes.\")\n",
    "            self.class_names = ('BACKGROUND', 'aeroplane', 'bicycle',\n",
    "                                'bird', 'boat', 'bottle', 'bus',\n",
    "                                'car', 'cat', 'chair', 'cow',\n",
    "                                'diningtable', 'dog', 'horse',\n",
    "                                'motorbike', 'person', 'pottedplant',\n",
    "                                'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "\n",
    "        self.class_dict = {class_name: i for i, class_name in enumerate(self.class_names)}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.ids[index]\n",
    "        image = self._read_image(image_id)\n",
    "        boxes, labels = self._get_annotation(image_id)\n",
    "\n",
    "        if self.transform:\n",
    "            image, boxes, labels = self.transform(image, boxes, labels)\n",
    "        if self.target_transform:\n",
    "            boxes, labels = self.target_transform(boxes, labels)\n",
    "        return image, boxes, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def _get_annotation(self, image_id):\n",
    "        annotation_file = os.path.join(self.root, self.dataset_type, f\"{image_id}.xml\")\n",
    "        objects = ET.parse(annotation_file).findall(\"object\")\n",
    "        boxes, labels = [], []\n",
    "\n",
    "        for obj in objects:\n",
    "            class_name = obj.find('name').text.lower().strip()\n",
    "            if class_name in self.class_dict:\n",
    "                bbox = obj.find('bndbox')\n",
    "                x1 = float(bbox.find('xmin').text) - 1\n",
    "                y1 = float(bbox.find('ymin').text) - 1\n",
    "                x2 = float(bbox.find('xmax').text) - 1\n",
    "                y2 = float(bbox.find('ymax').text) - 1\n",
    "                boxes.append([x1, y1, x2, y2])\n",
    "                labels.append(self.class_dict[class_name])\n",
    "\n",
    "        return np.array(boxes, dtype=np.float32), np.array(labels, dtype=np.int64)\n",
    "\n",
    "    def _read_image(self, image_id):\n",
    "        image_dir = self.dataset_type\n",
    "        image_file = os.path.join(self.root, image_dir, f\"{image_id}.jpg\")\n",
    "        if not os.path.exists(image_file):\n",
    "            image_file = os.path.join(self.root, image_dir, f\"{image_id}.png\")\n",
    "\n",
    "        image = cv2.imread(image_file)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________\n",
    "## IV. Training the student Neural Network: SSD MobileNet V2 for object detection\n",
    "With the labeled dataset prepared and verified, we can now proceed to train the student model. The student model will be trained using the annotations generated by the teacher model, leveraging the high-quality labels to learn and generalize object detection tasks. This training process involves feeding the pre-processed images and their corresponding labels into the student model, optimizing the model parameters through iterative learning. The goal is to achieve a model that performs efficiently and accurately in detecting objects, even with potentially fewer resources or simpler architecture compared to the teacher model. The following cell will initiate the training process, detailing the configuration, hyperparameters, and training loop necessary to build a robust student model. </br> The first step here will consist of generating the Anchor Boxes to be used with the Anchox Box matcher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generating the Anchor box for the SSD MobileNetV2 model:\n",
    "We start by defining the `AnchorBoxMatcher` class which is used to prepare ground truth data for training an object detection model by matching ground truth boxes to predefined anchor boxes and encoding the matched boxes into a format suitable for training. The following cell defines a class that implements this behavior which should be a transformation for targets later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the areas of rectangles given two corners.\n",
    "def area_of(left_top, right_bottom) -> torch.Tensor:\n",
    "    hw = torch.clamp(right_bottom - left_top, min=0.0)\n",
    "    return hw[..., 0] * hw[..., 1]\n",
    "\n",
    "def corner_form_to_center_form(boxes):\n",
    "    return torch.cat([(boxes[..., :2] + boxes[..., 2:]) / 2,\n",
    "                       boxes[..., 2:] - boxes[..., :2]], boxes.dim() - 1)\n",
    "\n",
    "def center_form_to_corner_form(locations):\n",
    "    return torch.cat([locations[..., :2] - locations[..., 2:]/2,\n",
    "                     locations[..., :2] + locations[..., 2:]/2], locations.dim() - 1)\n",
    "\n",
    "def iou_of(boxes0, boxes1, eps=1e-5):\n",
    "    overlap_left_top = torch.max(boxes0[..., :2], boxes1[..., :2])\n",
    "    overlap_right_bottom = torch.min(boxes0[..., 2:], boxes1[..., 2:])\n",
    "\n",
    "    overlap_area = area_of(overlap_left_top, overlap_right_bottom)\n",
    "    area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n",
    "    area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n",
    "    return overlap_area / (area0 + area1 - overlap_area + eps)\n",
    "\n",
    "def convert_boxes_to_locations(center_form_boxes, center_form_anchors, center_variance, size_variance):\n",
    "    # anchors can have one dimension less\n",
    "    if center_form_anchors.dim() + 1 == center_form_boxes.dim():\n",
    "        center_form_anchors = center_form_anchors.unsqueeze(0)\n",
    "    return torch.cat([\n",
    "        (center_form_boxes[..., :2] - center_form_anchors[...,\n",
    "         :2]) / center_form_anchors[..., 2:] / center_variance,\n",
    "        torch.log(center_form_boxes[..., 2:] /\n",
    "                  center_form_anchors[..., 2:]) / size_variance\n",
    "    ], dim=center_form_boxes.dim() - 1)\n",
    "\n",
    "# Assign ground truth boxes and targets to anchors.\n",
    "def assign_anchors(gt_boxes, gt_labels, corner_form_anchors, iou_threshold):\n",
    "    if gt_boxes.size(0) == 0:\n",
    "        gt_boxes = torch.zeros((1, 1, 4), dtype=torch.float32)\n",
    "        gt_labels = torch.zeros((1,), dtype=torch.int64)\n",
    "        no_gt = True\n",
    "    else:\n",
    "        no_gt = False\n",
    "    ious = iou_of(gt_boxes.unsqueeze(0), corner_form_anchors.unsqueeze(1))\n",
    "    best_target_per_anchor, best_target_per_anchor_index = ious.max(1)\n",
    "    best_anchor_per_target, best_anchor_per_target_index = ious.max(0)\n",
    "    \n",
    "    # Ensure best_anchor_per_target_index is a 1-dimensional tensor\n",
    "    best_anchor_per_target_index = best_anchor_per_target_index.view(-1)\n",
    "\n",
    "    for target_index, anchor_index in enumerate(best_anchor_per_target_index):\n",
    "        best_target_per_anchor_index[anchor_index] = target_index\n",
    "    best_target_per_anchor.index_fill_(0, best_anchor_per_target_index, 2)\n",
    "    if no_gt:\n",
    "        labels = torch.zeros((corner_form_anchors.size(0),), dtype=torch.int64)\n",
    "        boxes = torch.zeros((corner_form_anchors.size(0), 4), dtype=torch.float32)\n",
    "    else:\n",
    "        labels = gt_labels[best_target_per_anchor_index]\n",
    "        labels[best_target_per_anchor < iou_threshold] = 0  # the background id\n",
    "        boxes = gt_boxes[best_target_per_anchor_index]\n",
    "    return boxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnchorBoxMatcher(object):\n",
    "    def __init__(self, center_form_anchors, center_variance, size_variance, iou_threshold):\n",
    "        self.center_variance = center_variance\n",
    "        self.size_variance = size_variance\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.center_form_anchors = center_form_anchors\n",
    "        self.corner_form_anchors = center_form_to_corner_form(center_form_anchors)\n",
    "\n",
    "    def __call__(self, gt_boxes, gt_labels):\n",
    "        if type(gt_boxes) is np.ndarray:\n",
    "            gt_boxes = torch.from_numpy(gt_boxes)\n",
    "        if type(gt_labels) is np.ndarray:\n",
    "            gt_labels = torch.from_numpy(gt_labels)\n",
    "        boxes, labels = assign_anchors(gt_boxes, gt_labels, self.corner_form_anchors, self.iou_threshold)\n",
    "        boxes = corner_form_to_center_form(boxes)\n",
    "        locations = convert_boxes_to_locations(boxes, self.center_form_anchors, self.center_variance, self.size_variance)\n",
    "        return locations, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `generate_anchors` allows the generation predefined bounding boxes of various sizes and aspect ratios that are placed uniformly across the image. The function takes a list of specifications (specs) that define the feature map sizes, shrinkage, box sizes, and aspect ratios for each layer of the SSD model. It calculates the center coordinates, widths, and heights of the anchor boxes relative to the image size and returns them as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import List\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "AnchorBoxSizes = collections.namedtuple('AnchorBoxSizes', ['min', 'max'])\n",
    "AnchorSpec = collections.namedtuple( 'AnchorSpec', ['feature_map_size', 'shrinkage', 'box_sizes', 'aspect_ratios'])\n",
    "\n",
    "def generate_anchors(specs: List[AnchorSpec], image_size, clamp=True):\n",
    "    \"\"\"Generate SSD Anchor Boxes.\n",
    "    It returns the center, height and width of the anchor boxes. The values are relative to the image size\n",
    "    Args:\n",
    "        specs: AnchorSpec about the shapes of sizes of anchor boxes.image_size: image size.\n",
    "        clamp: if true, clamp the values to make fall between [0.0, 1.0]\n",
    "    Returns:\n",
    "        anchors (num_anchors, 4): The anchor boxes represented as [[center_x, center_y, w, h]]. All the values are relative to the image size.\n",
    "    \"\"\"\n",
    "    anchors = []\n",
    "    for spec in specs:\n",
    "        scale = image_size / spec.shrinkage\n",
    "        for j, i in itertools.product(range(spec.feature_map_size), repeat=2):\n",
    "            x_center = (i + 0.5) / scale\n",
    "            y_center = (j + 0.5) / scale\n",
    "\n",
    "            # small sized square box\n",
    "            size = spec.box_sizes.min\n",
    "            h = w = size / image_size\n",
    "            anchors.append([ x_center, y_center, w, h])\n",
    "\n",
    "            # big sized square box\n",
    "            size = math.sqrt(spec.box_sizes.max * spec.box_sizes.min)\n",
    "            h = w = size / image_size\n",
    "            anchors.append([x_center, y_center, w, h])\n",
    "\n",
    "            # change h/w ratio of the small sized box\n",
    "            size = spec.box_sizes.min\n",
    "            h = w = size / image_size\n",
    "            for ratio in spec.aspect_ratios:\n",
    "                ratio = math.sqrt(ratio)\n",
    "                anchors.append([x_center, y_center, w * ratio, h / ratio])\n",
    "                anchors.append([x_center, y_center, w / ratio, h * ratio])\n",
    "\n",
    "    anchors = torch.tensor(anchors)\n",
    "    if clamp:\n",
    "        torch.clamp(anchors, 0.0, 1.0, out=anchors)\n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ssd_input_size = 300\n",
    "ssd_specs = [\n",
    "    AnchorSpec(19, 16, AnchorBoxSizes(60, 105), [2, 3]),\n",
    "    AnchorSpec(10, 32, AnchorBoxSizes(105, 150), [2, 3]),\n",
    "    AnchorSpec(5, 64, AnchorBoxSizes(150, 195), [2, 3]),\n",
    "    AnchorSpec(3, 100, AnchorBoxSizes(195, 240), [2, 3]),\n",
    "    AnchorSpec(2, 150, AnchorBoxSizes(240, 285), [2, 3]),\n",
    "    AnchorSpec(1, 300, AnchorBoxSizes(285, 330), [2, 3])\n",
    "]\n",
    "\n",
    "center_variance = 0.1\n",
    "size_variance   = 0.2\n",
    "iou_threshold   = 0.45\n",
    "image_mean      = np.array([127, 127, 127])  # RGB layout \n",
    "image_std       = 128.0\n",
    "batch_size      = 4\n",
    "\n",
    "ssd_anchors = generate_anchors(specs=ssd_specs,  image_size=ssd_input_size)\n",
    "target_transform = AnchorBoxMatcher(ssd_anchors, center_variance, size_variance, iou_threshold)\n",
    "\n",
    "# Loading the training dataset using the dataloader from Torch\n",
    "train_transform = TransformData(size=ssd_input_size, mean=image_mean, std=image_std)\n",
    "train_dataset = VOCDataset(dataset_dir, transform=train_transform, target_transform=target_transform)\n",
    "train_loader  = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "\n",
    "# Loading the validation dataset using the dataloader from Torch\n",
    "valid_transform = TransformData(size=ssd_input_size, mean=image_mean, std=image_std) ###############################################################################\n",
    "valid_dataset = VOCDataset(dataset_dir, dataset_type= \"eval\", transform=valid_transform, target_transform=target_transform)\n",
    "valid_loader  = DataLoader(valid_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading the student model and the training artifacts:\n",
    "As mentionned ealier The **SSD MobileNet V2** model, also known as the student model, is previously trained on **PASCAL VOC** dataset and this use case serves only for model specialization to make models perform in high performance and accuracy in some tasks where the data of training is collected on device. </br> \n",
    "The first step consist of loading the SSD MobileNet V2 model along with the training artifacts which are the training, the evaluation and the optimizer subgraphs.  For that purpose, we use from the `onnxruntime-training` python module the classes `Module` to load the training and the eval graphs, `Optimizer` to load the optimizer and the `CheckpointState` to load the previously pre-trained weights if there are any. We start by defining the required training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime.training.api as orttraining\n",
    "\n",
    "# Training Parameters\n",
    "artifacts_dir_path = JUPYTER_ROOT_PATH + \"student_model/ssd_mobilenet_v2/training_artifacts/\"\n",
    "learning_rate = 0.005\n",
    "\n",
    "checkpoint_state = orttraining.CheckpointState.load_checkpoint(\n",
    "    f\"{artifacts_dir_path}checkpoint\")\n",
    "\n",
    "model = orttraining.Module(\n",
    "    f\"{artifacts_dir_path}training_model.onnx\",\n",
    "    checkpoint_state,\n",
    "    f\"{artifacts_dir_path}eval_model.onnx\",\n",
    ")\n",
    "\n",
    "optimizer = orttraining.Optimizer(\n",
    "    f\"{artifacts_dir_path}optimizer_model.onnx\", model\n",
    ")\n",
    "optimizer.set_learning_rate(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Launching the training loop along with model evaluation\n",
    "Now that all the objects and variables necessary for the training have been instanciated, we are set to launch the **training loop** as follows. The method `model.train()` set the model in training model by calling the training subgraph, the method `optimizer.step()` updates the model parameters based on the computed gradients.and This `model.lazy_reset_grad()` method sets the internal state of the module such that the module gradients will be scheduled to be reset just before the new gradients are computed on the next invocation of train()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, epoch, num_epochs):\n",
    "    losses = []\n",
    "    for i, data in enumerate(dataloader):\n",
    "        model.train()\n",
    "        images, boxes, labels = data\n",
    "        loss, confs, _ = model(np.array(images), np.array(labels).astype(np.float32), np.array(boxes))\n",
    "        optimizer.step()\n",
    "        model.lazy_reset_grad()\n",
    "        losses.append(loss.item())\n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the `evaluation loop`, we start by setting the model into eval mode and by looping around all the batches, we call the evaluation graph which returns the **evaluation loss** metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataloader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for i, data in enumerate(dataloader):\n",
    "        images, boxes, labels = data\n",
    "        loss, _, _ = model(np.array(images), np.array(labels).astype(np.float32), np.array(boxes))\n",
    "        losses.append(loss.item())\n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting the `num_epochs` variable to a certain value depending on the complexity of the use case, we are ready to launch the on-device learning loop by calling the functions `train()` and `eval()` successively at each pass of the `num_epochs`. You should notice the Validation Loss being lower than the Training Loss and both of them are decreasing as the training advances in epochs. Otherwise, your model should be overfitting due to dataset issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "for epoch in range(0, num_epochs):\n",
    "    train_loss = train(model=model,\n",
    "                       dataloader=train_loader, \n",
    "                       optimizer=optimizer, \n",
    "                       epoch=epoch, \n",
    "                       num_epochs=num_epochs)\n",
    "    val_loss = eval(model=model, dataloader=valid_loader)\n",
    "    print(f\"Epoch: {epoch + 1} / {num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Exporting model for inference\n",
    "After making sure that the training session is reaching quite satisfactory validation loss results, it is time to **export the model for inference**. This involves converting the trained model into a format that can be efficiently used for making predictions on new data. This process typically includes saving the model architecture, weights, and any necessary preprocessing steps into a deployable format such as ONNX. The exported model can then be loaded into an inference engine or runtime environment, where it can process input data and generate predictions in real-time or batch mode. </br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_models_dir = JUPYTER_ROOT_PATH + 'student_model/ssd_mobilenet_v2/inference_artifacts'\n",
    "\n",
    "if os.path.exists(f\"{inference_models_dir}/ssd_mobilenet_v2.onnx\"):\n",
    "    model.export_model_for_inferencing(f\"{inference_models_dir}/new_ssd_mobilenet_v2.onnx\", [\"confs\", \"out_boxes\"])\n",
    "    print(f\"Model exported to: {inference_models_dir}/new_ssd_mobilenet_v2.onnx\")\n",
    "else:\n",
    "    model.export_model_for_inferencing(f\"{inference_models_dir}/ssd_mobilenet_v2.onnx\", [\"confs\", \"out_boxes\"])\n",
    "    print(f\"Model exported to: {inference_models_dir}/ssd_mobilenet_v2.onnx.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________\n",
    "## V. Running inference using the newly exported model:\n",
    "Before running the inference using the new updated model on-device, it is necessary to apply some transformations on the model to make it run in optimal performances and AI hardware acceleration chip; We are talking here about the **NPU** (Neural Processing Unit) available on the STM32MP25 product family. Among these transformations we can mention the **static quantization** and **making the dynamic shapes fixed** to allow execution on the NPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Quantizing and optimizing the student model:\n",
    "Retraining a convolutional neural network is done generally in floating point format. In order to take advantage of the NPU acceleration, 8 bit linear quantization is required. For that purpose we are going to be using the **static quantization**.</br>\n",
    "The static quantization method initially executes the model with a set of inputs known as **calibration data**. Throughout these executions, the quantization parameters for each activation are calculated. These parameters are then embedded as constants in the quantized model and applied to all inputs. Our quantization tool offers support for three calibration methods: MinMax, Entropy, and Percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quant_pre_process, quantize_static, QuantFormat, QuantType, CalibrationDataReader\n",
    "import onnxruntime as ort\n",
    "\n",
    "def _preprocess_images(images_folder: str, height: int, width: int, nb_images=10):\n",
    "    unconcatenated_batch_data = []\n",
    "    img_counter = 0\n",
    "    for img_filename in sorted(os.listdir(images_folder)):\n",
    "        if img_counter > nb_images:\n",
    "            break\n",
    "        image = cv2.imread(images_folder + f\"/{img_filename}\")\n",
    "        if image is None:\n",
    "            continue\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image, _, _ = train_transform(image, np.empty((0, 4), dtype=np.float32), np.empty((0,), dtype=np.int32))\n",
    "        input_tensor = np.expand_dims(image, axis=0)\n",
    "        unconcatenated_batch_data.append(input_tensor)\n",
    "        img_counter += 1\n",
    "    batch_data = np.concatenate(np.expand_dims(unconcatenated_batch_data, axis=0), axis=0)\n",
    "    return batch_data\n",
    "\n",
    "class SSDDataReader(CalibrationDataReader):\n",
    "    def __init__(self, calibration_image_folder: str, model_path: str, nb_images: int):\n",
    "        self.enum_data = None\n",
    "\n",
    "        # Use inference session to get input shape.\n",
    "        session = ort.InferenceSession(model_path, None, providers=['CPUExecutionProvider'])\n",
    "        (_, _, height, width) = session.get_inputs()[0].shape\n",
    "\n",
    "        # Convert image to input data\n",
    "        self.nhwc_data_list = _preprocess_images(calibration_image_folder, height, width, nb_images)\n",
    "        self.input_name = session.get_inputs()[0].name\n",
    "        self.datasize = len(self.nhwc_data_list)\n",
    "\n",
    "    def get_next(self):\n",
    "        if self.enum_data is None:\n",
    "            self.enum_data = iter(\n",
    "                [{self.input_name: nhwc_data}\n",
    "                    for nhwc_data in self.nhwc_data_list]\n",
    "            )\n",
    "        return next(self.enum_data, None)\n",
    "\n",
    "    def rewind(self):\n",
    "        self.enum_data = None\n",
    "\n",
    "######## Quantization ########\n",
    "calib_dir_path = dataset_dir + \"/train/\"\n",
    "nb_images_calib = num_samples_images * 0.4\n",
    "\n",
    "if os.path.exists(f\"{inference_models_dir}/new_ssd_mobilenet_v2.onnx\"):\n",
    "    float_model_path = f\"{inference_models_dir}/new_ssd_mobilenet_v2.onnx\"\n",
    "else:\n",
    "    float_model_path = f\"{inference_models_dir}/ssd_mobilenet_v2.onnx\"\n",
    "\n",
    "# Preprocessing before quantization\n",
    "quant_pre_process(float_model_path, f\"{inference_models_dir}/new_ssd_mobilenet_v2_pp.onnx\")\n",
    "\n",
    "# Static quantization\n",
    "quant_model_name = \"new_ssd_mobilenet_v2_quant\"\n",
    "datareader = SSDDataReader(calib_dir_path, float_model_path, nb_images=nb_images_calib)\n",
    "\n",
    "quantize_static(f\"{inference_models_dir}/new_ssd_mobilenet_v2_pp.onnx\",\n",
    "                f\"{inference_models_dir}/{quant_model_name}.onnx\",\n",
    "                calibration_data_reader=datareader, activation_type=QuantType.QInt8,\n",
    "                weight_type=QuantType.QInt8, quant_format=QuantFormat.QDQ,\n",
    "                per_channel=True, reduce_range=True)\n",
    "\n",
    "print(\"Quantization done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice the generation of a new model `new_ssd_mobilenet_v2_quant.onnx` in your filesystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Making dynamic input shapes fixed:\n",
    "If an ONNX model can potentially be used with VSINPU Execution Provider as reported by the model usability checker, it may benefit from making the input shapes fixed. This is because VSINPU EP does not support dynamic input shapes. Fixing the dynamic shape simply means making the batch size dimension fixed by setting it to 1 to allow the model to run on the NPU hardware inference accelerator. ONNXRuntime provides a tool that allows making the dynamic shapes fixed for an ONNX model by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m onnxruntime.tools.make_dynamic_shape_fixed --dim_param batch --dim_value 1 student_model/ssd_mobilenet_v2/inference_artifacts/new_ssd_mobilenet_v2_quant.onnx student_model/ssd_mobilenet_v2/inference_artifacts/ssd_mobilenet_v2_quant_fixed.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining the SSD MobileNet V2 post process function:\n",
    "The function `convert_locations_to_boxes` takes care of converting regressional location results of SSD into boxes in the form of **(center_x, center_y, h, w)**. It takes as an argument the locations retrieved from the output tensor of the model, the anchors and two float parameters representing the center_variance and the size_variance. It returns the values of the boxes coordinates relative to the image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_locations_to_boxes(locations, priors, center_variance, size_variance):\n",
    "    # priors can have one dimension less.\n",
    "    if len(priors.shape) + 1 == len(locations.shape):\n",
    "        priors = np.expand_dims(priors, 0)\n",
    "    return np.concatenate([\n",
    "        locations[..., :2] * center_variance *\n",
    "        priors[..., 2:] + priors[..., :2],\n",
    "        np.exp(locations[..., 2:] * size_variance) * priors[..., 2:]\n",
    "    ], axis=len(locations.shape) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hard_nms` function performs **Hard Non-Maximum Suppression (NMS)** on a set of bounding boxes with associated scores. It takes in a list of bounding boxes and their scores, an Intersection over Union (IoU) threshold, a parameter top_k to limit the number of results, and a candidate_size to consider only the top-scoring candidates. The function sorts the boxes by their scores, then iteratively selects the box with the highest score, removes it from the list, and suppresses all other boxes that have an IoU greater than the specified threshold with the selected box. This process continues until the desired number of boxes (top_k) is selected or all candidates are processed. The function returns the list of selected boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_nms(box_scores, iou_threshold, top_k=-1, candidate_size=200):\n",
    "    scores = box_scores[:, -1]\n",
    "    boxes = box_scores[:, :-1]\n",
    "    picked = []\n",
    "    indexes = np.argsort(scores)\n",
    "    indexes = indexes[-candidate_size:]\n",
    "    while len(indexes) > 0:\n",
    "        current = indexes[-1]\n",
    "        picked.append(current)\n",
    "        if 0 < top_k == len(picked) or len(indexes) == 1:\n",
    "            break\n",
    "        current_box = boxes[current, :]\n",
    "        indexes = indexes[:-1]\n",
    "        rest_boxes = boxes[indexes, :]\n",
    "        iou = iou_of(rest_boxes,  torch.from_numpy(np.expand_dims(current_box, axis=0)))\n",
    "        indexes = indexes[iou <= iou_threshold]\n",
    "\n",
    "    return box_scores[picked, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define next the `postprocess_ssdmobilenetv2` function which applies the `convert_locations_to_boxes` and the `hard_nms` on the output tensors of the **SSD MobileNet V2** model. The expected results are numpy arrays containing the boxes coordinates adjusted to the preview image, the confidences of each of the classes (BACKGROUND and Person in this case) and the class IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_ssdmobilenetv2(scores, boxes, conf_threshold, iou_threshold, preview_shape):\n",
    "        preview_image_width = preview_shape[0]\n",
    "        preview_image_height = preview_shape[1]\n",
    "        # Apply softmax to the scores\n",
    "        scores = np.exp(scores) / np.sum(np.exp(scores), axis=2, keepdims=True)\n",
    "        boxes = convert_locations_to_boxes(boxes, ssd_anchors, 0.1, 0.2)\n",
    "        boxes = np.array(center_form_to_corner_form(torch.from_numpy(boxes)))\n",
    "        boxes = np.array(boxes[0])\n",
    "        scores = np.array(scores[0])\n",
    "        picked_box_probs = []\n",
    "        picked_labels = []\n",
    "        for class_index in range(1, scores.shape[1]):\n",
    "            probs = scores[:, class_index]\n",
    "            mask = probs > conf_threshold\n",
    "            probs = probs[mask]\n",
    "            if probs.shape[0] == 0:\n",
    "                continue\n",
    "            subset_boxes = boxes[mask, :]\n",
    "            box_probs = np.concatenate([subset_boxes, probs.reshape(-1, 1)], axis=1)\n",
    "\n",
    "            box_probs = hard_nms(torch.from_numpy(box_probs), iou_threshold=iou_threshold)\n",
    "            picked_box_probs.append(box_probs)\n",
    "            picked_labels.extend([class_index] * box_probs.shape[0])\n",
    "\n",
    "        if not picked_box_probs:\n",
    "            picked_box_probs = np.array([])\n",
    "            boxes = np.empty((0, 4))\n",
    "        else:\n",
    "            picked_box_probs = np.concatenate(picked_box_probs)\n",
    "            picked_box_probs[:, 0] *= preview_image_width\n",
    "            picked_box_probs[:, 1] *= preview_image_height\n",
    "            picked_box_probs[:, 2] *= preview_image_width\n",
    "            picked_box_probs[:, 3] *= preview_image_height\n",
    "            boxes = picked_box_probs[:, :4]\n",
    "            probs = picked_box_probs[:, 4]\n",
    "        return boxes, probs, picked_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Running the inference using the STAI_MPU API with NPU acceleration:\n",
    "The last step of this tutorial is to run inference using the newly updated model after being quantized and have its dynamic shapes fixed using the STAI_MPU API and the NPU hardware acceleration. We use the the `postprocess_ssdmobilenetv2` function to feed the boxes coordinates, the confidences and the class ID to the Supervision `Detections` class. As for the preprocessing, we apply the same function as the Teacher model but with different image size, since they consume similar data with different resolutions. We use the `display_annotated_images` function defined previously to display the detection results of the new student model. You should notice improved detection performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stai_mpu import stai_mpu_network\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Instantiate the student ONNX model with the use_hw_acceleration flag\n",
    "student_model_path = inference_models_dir + \"/ssd_mobilenet_v2_quant_fixed.onnx\"\n",
    "stai_student_model = stai_mpu_network(model_path=student_model_path, use_hw_acceleration=True)\n",
    "\n",
    "# Read input tensor information\n",
    "num_inputs = stai_student_model.get_num_inputs()\n",
    "input_tensor_infos = stai_student_model.get_input_infos()\n",
    "input_tensor_shape = input_tensor_infos[0].get_shape()\n",
    "input_tensor_dtype = input_tensor_infos[0].get_dtype()\n",
    "nn_input_width =  input_tensor_shape[2]\n",
    "nn_input_height =  input_tensor_shape[3]\n",
    "nn_input_channel =  input_tensor_shape[1]\n",
    "\n",
    "# Read output tensor information\n",
    "num_outputs = stai_student_model.get_num_outputs()\n",
    "output_tensor_infos = stai_student_model.get_output_infos()\n",
    "output_tensor_shape = output_tensor_infos[0].get_shape()\n",
    "\n",
    "# Filtering parameters\n",
    "conf_threshold = 0.7\n",
    "iou_threshold = 0.3\n",
    "\n",
    "def run_inference(image_paths):\n",
    "    detections_list = []\n",
    "    for img_path in image_paths:\n",
    "        img = cv2.imread(img_path)\n",
    "        preprocessed_img = preprocess_input(img, nn_input_width, nn_input_height)\n",
    "        stai_student_model.set_input(0, np.array(preprocessed_img))\n",
    "        # Run inference using the STAI_MPU on the newly exported model\n",
    "        start_time = time.time()\n",
    "        stai_student_model.run()\n",
    "        print(f'Inference time for {img_path}: {time.time() - start_time}\\n')\n",
    "        output_tensor_conf = stai_student_model.get_output(index=0)\n",
    "        output_tensor_bbox = stai_student_model.get_output(index=1)\n",
    "        boxes, scores, class_ids = postprocess_ssdmobilenetv2(output_tensor_conf,\n",
    "                                                              output_tensor_bbox,\n",
    "                                                              conf_threshold,\n",
    "                                                              iou_threshold,\n",
    "                                                              img.shape)\n",
    "        detections = sv.Detections(xyxy=boxes, confidence=scores, class_id=np.array(class_ids))\n",
    "        detections_list.append(detections)\n",
    "    return detections_list\n",
    "\n",
    "image_paths = glob.glob(f\"{dataset_dir}/test/*.jpg\")\n",
    "detections_list = run_inference(image_paths[:10])\n",
    "display_annotated_images(image_paths[:10], detections_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be noticed, the model can You are welcome to deploy your new model to an object detection application running in real time video stream with Gstreamer and the STAI_MPU API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
